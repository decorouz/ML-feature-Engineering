{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "81e71525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"wrapper_methods_project/data/student/student-mat.csv\", sep=\";\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "08ae3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the variables\n",
    "data = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f11f3c",
   "metadata": {},
   "source": [
    "We set our predictor and outcome variables and perform a train-test-split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7b8964f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(columns=[\"G3\"])\n",
    "y = data[\"G3\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=59)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582b6ee",
   "metadata": {},
   "source": [
    "We’re now ready to fit a Lasso regularized regression model by using the `Lasso()` function from the linear_model module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "86c4747d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.05)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.05)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919c88b",
   "metadata": {},
   "source": [
    "We’re going to look at the Mean Squared Error (MSE) using a function from the `metrics` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fd7ee726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 3.1166850450733663 \n",
      "Test Error: 4.107656922299011 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_train = lasso.predict(X_train)\n",
    "pred_test = lasso.predict(X_test)\n",
    "training_mse = mean_squared_error(y_train, pred_train)\n",
    "test_mse = mean_squared_error(y_test, pred_test)\n",
    "\n",
    "print(f\"Training Error: {training_mse} \")\n",
    "print(f\"Test Error: {test_mse} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d2e7d",
   "metadata": {},
   "source": [
    "We see that the test error is more than that of the training error. This is an indicator that the model is still overfitting the data and that there is room to regularize more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff260439",
   "metadata": {},
   "source": [
    "## Tuning the Regularization Hyperparameter\n",
    "**Automate the Hyperparameter Search with GridsearchCV**:\n",
    "\n",
    "`GridSearchCV` uses a “k-fold” cross-validation method to search for the optimal hyperparemeter in a machine learning algorithm. The k-fold method iteratively splits the dataset into train-test splits “k” times such that every point in the sample gets to be within the test data at least once. For instance, if we wanted to do 5-fold cross-validation, our data would be split into a 80:20 train-test split five times. To visualize this, imagine that our data is contained within this green box as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474ef6",
   "metadata": {},
   "source": [
    "Consider our implementation of Lasso on the student performance dataset. Suppose we wanted to check the result of implementing different values of alpha and wanted to do multiple train-test splits to make sure we’re covering the entire sample. We could do this in one line as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d495f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## an array of alpha values between 0.000001 and 1.0\n",
    "alpha_array = np.logspace(-6, 0, 100)\n",
    " \n",
    "#dict with key (alpha) and values being alpha_array\n",
    "tuned_parameters = [{'alpha': alpha_array}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bcb441d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.039e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.039e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.442e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.172e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.173e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.173e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.173e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+02, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.895e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.266e+01, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.896e+00, tolerance: 6.643e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.266e+01, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.266e+01, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.137e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.040e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.225e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.225e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.252e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.225e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.252e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.225e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.252e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.252e+01, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.154e+01, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.265e+00, tolerance: 5.921e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.638e+00, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+00, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.142e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+00, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.142e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e+00, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.143e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.641e+00, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.144e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.641e+00, tolerance: 6.401e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.145e+00, tolerance: 7.172e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.228e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/user/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.228e+01, tolerance: 6.885e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Lasso(alpha=0.05),\n",
       "             param_grid=[{'alpha': array([1.00000000e-06, 1.14975700e-06, 1.32194115e-06, 1.51991108e-06,\n",
       "       1.74752840e-06, 2.00923300e-06, 2.31012970e-06, 2.65608778e-06,\n",
       "       3.05385551e-06, 3.51119173e-06, 4.03701726e-06, 4.64158883e-06,\n",
       "       5.33669923e-06, 6.13590727e-06, 7.05480231e-06, 8.11130831e-06,\n",
       "       9.32603347e-06, 1.07226722e-05, 1.2328...\n",
       "       7.05480231e-02, 8.11130831e-02, 9.32603347e-02, 1.07226722e-01,\n",
       "       1.23284674e-01, 1.41747416e-01, 1.62975083e-01, 1.87381742e-01,\n",
       "       2.15443469e-01, 2.47707636e-01, 2.84803587e-01, 3.27454916e-01,\n",
       "       3.76493581e-01, 4.32876128e-01, 4.97702356e-01, 5.72236766e-01,\n",
       "       6.57933225e-01, 7.56463328e-01, 8.69749003e-01, 1.00000000e+00])}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GridSearchCV(estimator=lasso, param_grid= tuned_parameters, \n",
    "                     scoring=\"neg_mean_squared_error\", cv=5, return_train_score= True)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6a870028",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.cv_results_[\"mean_test_score\"]\n",
    "train_scores = model.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3ef37534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.12328467394420659} -3.743294901559114\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_, model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c9ccd",
   "metadata": {},
   "source": [
    "### Implementing Regularization with a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486875c4",
   "metadata": {},
   "source": [
    "Implementing Regularization Methods in Python\n",
    "Learn how to implement L1 and L2 regularization using scikit-learn and tune the regularization hyperparameter using GridSearchCV for linear and logistic regression.\n",
    "\n",
    "Implementing Regularization with Linear Regression\n",
    "In the previous lesson, we saw how implementing Lasso and Ridge regularization methods with linear regression in scikit-learn was fairly straightforward. Both Lasso() and Ridge() functions have an input parameter alpha that sets the strength of the regularization. The not-so-straightforward part was the question of how to choose the right alpha for the data and question at hand. We’re now ready to dive into this!\n",
    "\n",
    "For starters, we’re going run a quick Lasso implementation on the student performance dataset that we were working with.The dataset has 41 features relating to various factors that might influence a student’s final grade. Let’s take a sneak peek first:\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('student_math.csv')\n",
    "print(df.columns, df.shape)\n",
    "Output:\n",
    "\n",
    "Index(['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',\n",
    "       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2',\n",
    "       'Final_Grade', 'school_MS', 'sex_M', 'address_U', 'famsize_LE3',\n",
    "       'Pstatus_T', 'Mjob_health', 'Mjob_other', 'Mjob_services',\n",
    "       'Mjob_teacher', 'Fjob_health', 'Fjob_other', 'Fjob_services',\n",
    "       'Fjob_teacher', 'reason_home', 'reason_other', 'reason_reputation',\n",
    "       'guardian_mother', 'guardian_other', 'schoolsup_yes', 'famsup_yes',\n",
    "       'paid_yes', 'activities_yes', 'nursery_yes', 'higher_yes',\n",
    "       'internet_yes', 'romantic_yes'],\n",
    "      dtype='object') (395, 42)\n",
    "We set our predictor and outcome variables and perform a train-test-split:\n",
    "\n",
    "y = df['Final_Grade']\n",
    "X = df.drop(columns = ['Final_Grade'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "We’re now ready to fit a Lasso regularized regression model by using the Lasso() function from the linear_model module:\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha = 0.05)\n",
    "lasso.fit(X_train,y_train)\n",
    "We’re going to look at the Mean Squared Error (MSE) using a function from the metrics module:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pred_train = lasso.predict(X_train)\n",
    "pred_test = lasso.predict(X_test)\n",
    "training_mse = mean_squared_error(y_train, pred_train)\n",
    "test_mse = mean_squared_error(y_test, pred_test)\n",
    "print('Training Error:',  training_mse)\n",
    "print('Test Error:', test_mse)\n",
    "Output:\n",
    "\n",
    "Training Error: 2.8132075838851405\n",
    "Test Error: 4.474769444129441\n",
    "We see that the test error is more than 1.5 times that of the training error. This is an indicator that the model is still overfitting the data and that there is room to regularize more.\n",
    "\n",
    "Tuning the Regularization Hyperparameter\n",
    "We chose alpha to be 0.05 (the scikit-learn default is 1.0). It is worth recalling here that an alpha of zero is equivalent to no regularization. And when alpha is allowed to get too big we have the opposite problem of biasing our model and consequently underfitting the data.\n",
    "\n",
    "One way to figure out the “Goldilocks zone” in this bias-variance tradeoff is to try multiple values of alpha. Iterating over an array of alpha values and plotting the resulting training and test errors against the corresponding alpha‘s gets us the following figure:\n",
    "\n",
    "image\n",
    "\n",
    "We see that the test error goes down as we increase alpha until a certain value after which it begins to go up and then seems to slowly but steadily increase. Our optimal alpha thus lies somewhere in the blue band shown, likely a number between 0.12 and 0.16.\n",
    "\n",
    "There is, however, another question to consider: would this be the same for a different train-test split? It is quite possible that our tuned hyperparameter might be different if we did a different train-test split. Luckily, scikit-learn has just the feature for us. GridSearchCV, short for “Grid Search Cross-validation” helps us to satisfactorily resolve this while automating and speeding up our hyperparameter search - all within a couple of lines of code!\n",
    "\n",
    "Automate the Hyperparameter Search with GridsearchCV\n",
    "GridSearchCV uses a “k-fold” cross-validation method to search for the optimal hyperparemeter in a machine learning algorithm. The k-fold method iteratively splits the dataset into train-test splits “k” times such that every point in the sample gets to be within the test data at least once. For instance, if we wanted to do 5-fold cross-validation, our data would be split into a 80:20 train-test split five times. To visualize this, imagine that our data is contained within this green box as shown below:\n",
    "\n",
    "kfold\n",
    "\n",
    "Our five “folds” can be represented by the yellow box. Each location of the box shows a specific train-test split. If we iterated our model fit over these five folds, every part of the data gets to be “test data” once. Doing a k-fold cross-validation makes sure that the conclusions we’re drawing about our data are not the result of a sampling effect but are truly representative of all of the data.\n",
    "\n",
    "GridSearchCV takes in the following arguments:\n",
    "\n",
    "estimator: the machine learning algorithm whose hyperparameters we’re tuning (in our case, Lasso() or Ridge())\n",
    "\n",
    "param_grid: a grid of potential values for the hyperparameters that are being tuned. This has to be in the form of a dictionary with the keys representing the paramater inputs to the model and the values, lists of potential parameter values. For instance in the case of our Lasso implementation, we have only one hyperparameter we’re tuning, i.e., alpha.\n",
    "\n",
    "Since we don’t have an idea of what order of magnitude alpha needs to be (i.e., is it between 0.01-0.1 or 0.1-1.0 or 1-10 and so on), using NumPy‘s logspace function can be very useful. It is similar to linspace except that it gets us numbers that are evenly spaced in the logarithmic scale. (It takes in the powers of ten that we’re searching between, so if we wanted to search between 0.01 to 100, we’d set its inputs to -2 and 2 respectively.)\n",
    "\n",
    "import numpy as np\n",
    "## an array of alpha values between 0.000001 and 1.0\n",
    "alpha_array = np.logspace(-6, 0, 100)\n",
    " \n",
    "#dict with key (alpha) and values being alpha_array\n",
    "tuned_parameters = [{'alpha': alpha_array}]\n",
    "scoring: the metric used to evaluate the performance of the model on the test set. GridSearchCV searches for the set of parameters within param_grid that maximizes this value.\n",
    "scikit-learn‘s documentation has an exhaustive list of scoring strategies. The argument for mean squared error, our chosen metric, is neg_mean_squared_error. It’s a negative value because GridSearchCV maximizes the score it’s given.\n",
    "\n",
    "cv: specifies the way the cross-validation is performed. The default here is the 5-fold cross-validation method that’s described above. Otherwise one can set a different number of folds (i.e., an alternate positive integer value for k)\n",
    "\n",
    "return_train_score: a Boolean argument specifying whether we would like the output of our fit to return the scores on training data every folds The default here is False but we’re going to set it to True as we’re using GridSearchCV to correct for overfitting and would like to be kept in the loop about the training score.\n",
    "\n",
    "Consider our implementation of Lasso on the student performance dataset. Suppose we wanted to check the result of implementing different values of alpha and wanted to do multiple train-test splits to make sure we’re covering the entire sample. We could do this in one line as follows:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GridSearchCV(estimator = Lasso(), param_grid = tuned_parameters, scoring = 'neg_mean_squared_error', cv = 5, return_train_score = True)\n",
    "model.fit(X, y)\n",
    "The result of the fit object has many attributes, of which we’re going to examine the ones that are most important to us:\n",
    "\n",
    "The cv_results_ object gives us the details of every model fit corresponding to a particular alpha value and the train-test split of each fold. (Our alpha array had a 100 values and we did a 5-fold cross-validated search - so this is essentially equivalent to performing 500 model fits!) We’re specifically going to look at the mean train and test scores across the 5 train-test splits:\n",
    "test_scores = model.cv_results_['mean_test_score']\n",
    "train_scores = model.cv_results_['std_test_score']\n",
    "Each of the above objects is an array the size of our param_grid, i.e., the number of alpha values we’ve specified.\n",
    "\n",
    "Additionally, the model fit object also gives us: best_estimator_, best_score_ and best_params_. To get the alpha that is optimal to our scoring strategy, we can do the following:\n",
    "print(model.best_params_, model.best_score_)\n",
    "Output:\n",
    "\n",
    "{'alpha': 0.12328467394420659} -3.7432949015591155\n",
    "(Note that the score is the negative of least test mean squared error!) If we were to replicate the previous plot, i.e., plot our new training and test scores as a function of alpha and over plot the line corresponding to the “best” alpha value, we get the following:\n",
    "\n",
    "cv\n",
    "\n",
    "The blue line here is our tuned hyperparameter and this value of alpha (~0.1233) corresponds to the optimal cross-validated test and training error values.\n",
    "\n",
    "Code Challenge\n",
    "We’ve loaded the student performance dataset here. Following the same logic as described above, implement hyperparameter tuning using GridSearchCV for Linear Regression with Ridge Regularization. Find the optimal alpha and the mean squared error corresponding to the best alpha. GridSearchCV might take about a minute to run - it is, after all, performing 5 (folds) times 100 (hyperparameter values) = 500 model fits! :)\n",
    "\n",
    "Note: We’ve already specified a range of alpha values between 0.01 and 10000. These are a few orders of magnitude more than Lasso’s alpha values. Can you think of why this might be the case?\n",
    "\n",
    "131415121110987654321\n",
    "alpha_array\n",
    "tuned_parameters = None\n",
    "\n",
    "# Perform GridSearchCV with Ridge regularization on the data\n",
    "\n",
    "\n",
    "# Print the tuned alpha and the best test score corresponding to it\n",
    "\n",
    "Output:\n",
    " \n",
    "Note: The alpha values are orders of magnitude bigger for Ridge for the following reason: alpha is inversely proportional to the size of the regularization constraint. Recall that for Lasso this means that it’s proportional to 1/s while for Ridge, this would be 1/s^2 as the constraint surfaces are different. So reducing s by a factor of 10 means increasing alpha 10x for Lasso but 100x for Ridge!\n",
    "\n",
    "Implementing Regularization with a Logistic Regression Classifier\n",
    "So far we’ve seen different implementations of regularization with linear regression. What if we were working with a classification problem instead? We’re going to discuss a couple of ways of implementing L1 and L2 regularization with a logistic regression classifier.\n",
    "\n",
    "On alpha‘s, C‘s and solver‘s: some scikit-learn pointers!\n",
    "This might come as a suprise to you (it certainly did to me): the default scikit-learn logistic regression implementation is already regularized! It is L2-regularized with an alpha of 100. To elaborate, let’s take a look at a couple of attributes that go into the logistic regression model:\n",
    "\n",
    "penalty: The options here are ‘l1’, ‘l2’, ‘none’ and ‘elasticnet’ and the default is ‘l2’.\n",
    "\n",
    "C: The inverse of regularization strength - the default value is 1.0.\n",
    "\n",
    "solver: The options are {‘lbfgs’, ‘liblinear’, ‘newton_cg’, ‘sag’, ‘saga’}.\n",
    "\n",
    "The LogisticRegression() function differs from Lasso() and Ridge() functions for Linear Regression in that its input is the parameter C, which is the inverse of alpha. This is important to keep in mind, especially while setting up the parameter grid prior to implementing GridSearchCV.\n",
    "\n",
    "Let’s take a look at our different options for implementation then:\n",
    "\n",
    "No regularization:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_no_regularization = LogisticRegression(penalty = 'none')\n",
    "Lasso (L1): The L1 implementation requires setting the penalty attribute to ‘l1’ and also setting the solver attribute to ‘liblinear’\n",
    "\n",
    "logistic_lasso = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = ___ )\n",
    "It is worth noting here that there is a solver attribute in scikit-learn for most machine learning algorithms. The default solver for LogisticRegression() is lbfgs, however the only solver that can be used with Lasso Regularization is the ‘liblinear’ solver and hence it needs to be explicitly specified. (We’re not going to go into further details at the moment but the docs are always a great place to get some more pointers!)\n",
    "\n",
    "Ridge (L2): Remember the default is ‘l2’! So all we need to do is specify C here:\n",
    "\n",
    "logistic_ridge = LogisticRegression(C = ___ )\n",
    "Elasticnet: The penalty options ‘l1’, ‘l2’ and ‘none’ are self-evident but we haven’t covered ‘elasticnet’ in this course. Elasticnet Regularization is a combination of L1 and L2 regularization and has two penalty terms, one for L1 and one for L2!\n",
    "\n",
    "logistic_elasticnet = LogisticRegression(penalty = 'elasticnet', solver = 'saga', C = ___, l1_ratio = ___ )\n",
    "To implement Elasticnet one needs two hyperparameters: C, which specifies the regularization strength and an additional mixing hyperparameter, ‘l1_ratio’ which specifies how much L1 regularization used relative to L2.\n",
    "‘l1_ratio’ takes values between 0 and 1, with 1 being the same as applying just L1 and 0, being equivalent to applying just L2 penalties respectively.\n",
    "The solver ‘saga’ needs to be explicitly specified here.\n",
    "Tuning hyperparameter C using GridSearchCV and LogisticRegressionCV\n",
    "Implementing Lasso or Ridge with logistic regression in scikit-learn is as simple as picking the right arguments for penalty and C. We’re written some sample code here to find the optimal C using GridSearchCV: (Remember that C is the inverse of alpha so greater the C, the lesser the amount of regularization!)\n",
    "\n",
    "#Making an array of C's; here we're choosing 100 values between 0.001 and 100\n",
    "C_array  = np.logspace(-3,2, 100)\n",
    " \n",
    "#Making a dict to enter as an input to param_grid\n",
    "tuning_C = {'C':C_array}\n",
    "clf = LogisticRegression(penalty = 'l1', solver =  'liblinear')\n",
    "gs = GridSearchCV(clf, param_grid = tuning_C, scoring = 'accuracy', cv = 5)\n",
    "Code Challenge\n",
    "A dataset related to different types of candies and their rankings has been loaded for you here. The data comes from FiveThirtyEight and the article it pertains to is Halloween Candy Rankings from 2017.\n",
    "\n",
    "But we’re going to answer a different question: can we predict whether a candy contains chocolate or not? Build a logistic regression classifier with L2 regularization using GridSearchCV to obtain the best C as well as the best possible accuracy here.\n",
    "\n",
    "1234567891011\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('candy-data.csv')\n",
    "y = df['chocolate']\n",
    "X = df.drop(columns = ['chocolate', \n",
    "Output:\n",
    " \n",
    "Since Logistic Regression is such a commonly used algorithm, scikit-learn has a function that makes the above faster and neater, the LogisticRegressionCV() function. If we were to implement the code challenge using this function, it would look as follows:\n",
    "\n",
    "model = LogisticRegressionCV( Cs=np.logspace(-3,2, 100),\n",
    "                                  penalty='l2',\n",
    "                                  scoring='accuracy', cv=5,\n",
    "                                  random_state=42,max_iter=10000)\n",
    "model.fit(X, y)\n",
    "print(model.C_, model.scores_[1].mean(axis=0).max())\n",
    "Output:\n",
    "\n",
    "[2.42012826] 0.8823529411764705\n",
    "Which is the same output as we get from the code challenge! Note that there are slight syntactical differences between the outputs of GridsearchCV and LogisticRegressionCV. In general, many algorithms in scikit-learn have their own special grid search cross-validation function such as LassoCV(), RidgeCV(), ElasticnetCV(), etc.\n",
    "\n",
    "Conclusion\n",
    "We have covered multiple ways of implementing regularization with linear and logistic regression in scikit-learn in this article. There are a few essential (albeit annoying) differences between these methods. Together they offer a powerful toolkit to find the optimal regularization for regression and classification methods. You can try out all these techniques in the upcoming project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53cbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91a68c6",
   "metadata": {},
   "source": [
    "# Logistic Regression with Regularization\n",
    "\n",
    "The data you’re going to be working with is from the [Wine Quality Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/) in the UCI Machine Learning Repository. We’re looking at the red wine data in particular and while the original dataset has a 1-10 rating for each wine, we’ve made it a classification problem with a wine quality of good (>5 rating) or bad (<=5 rating). The goals of this project are to:\n",
    "\n",
    "1. implement different logistic regression classifiers\n",
    "2. find the best ridge-regularized classifier using hyperparameter tuning\n",
    "3. implement a tuned lasso-regularized feature selection method\n",
    "\n",
    "What we’re working with:\n",
    "\n",
    "* 11 input variables (based on physicochemical tests): ‘fixed acidity’, ‘volatile acidity’, ‘citric acid’, ‘residual sugar’,’chlorides’, ‘free sulfur dioxide’, ‘total sulfur dioxide’, ‘density’, ‘pH’, ‘sulphates’ and ‘alcohol’.\n",
    "* An output variable, ‘quality’ (0 for bad and 1 for good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1574f0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "wine_df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "wine_df[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16c676",
   "metadata": {},
   "source": [
    "Make the wine quality label a classification feature. Good (>5 rating) or bad(<=5 rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29b679be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     855\n",
       "False    744\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = wine_df[\"quality\"] > 5\n",
    "wine_df[\"quality\"] = good\n",
    "wine_df[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd356b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecademylib3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_csv('wine_quality.csv')\n",
    "print(df.columns)\n",
    "y = df['quality']\n",
    "features = df.drop(columns = ['quality'])\n",
    "\n",
    "\n",
    "## 1. Data transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler_fit = StandardScaler().fit(features)\n",
    "X = standard_scaler_fit.transform(features)\n",
    "\n",
    "## 2. Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X,y, random_state = 99, test_size = 0.2\n",
    ")\n",
    "\n",
    "## 3. Fit a logistic regression classifier without regularization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_no_reg = LogisticRegression(penalty=\"none\")\n",
    "clf_no_reg.fit(X_train, y_train)\n",
    "\n",
    "## 4. Plot the coefficients\n",
    "predictors = features.columns\n",
    "coefficients = clf_no_reg.coef_.ravel()\n",
    "\n",
    "coef = pd.Series(coefficients, predictors).sort_values()\n",
    "\n",
    "coef.plot(kind=\"bar\", title = \"Coefficients (no regularization)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "## 5. Training and test performance\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred_test = clf_no_reg.predict(X_test)\n",
    "y_pred_train = clf_no_reg.predict(X_train)\n",
    "print('Training Score', f1_score(y_train, y_pred_train))\n",
    "print('Testing Score', f1_score(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "## 6. Default Implementation (L2-regularized!)\n",
    "clf_default = LogisticRegression()\n",
    "clf_default.fit(X_train, y_train)\n",
    "\n",
    "## 7. Ridge Scores\n",
    "y_pred_test = clf_default.predict(X_test)\n",
    "y_pred_train = clf_default.predict(X_train)\n",
    "print('Ridge-regularized Training Score', f1_score(y_train, y_pred_train))\n",
    "print('Ridge-regularized Testing Score', f1_score(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "## 8. Coarse-grained hyperparameter tuning\n",
    "training_array = []\n",
    "test_array = []\n",
    "C_array = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for x in C_array:\n",
    "  clf = LogisticRegression(C = x)\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred_test = clf.predict(X_test)\n",
    "  y_pred_train = clf.predict(X_train)\n",
    "  training_array.append(f1_score(y_train, y_pred_train))\n",
    "  test_array.append(f1_score(y_test, y_pred_test))\n",
    "\n",
    "    \n",
    "\n",
    "# 9. Plot training and test scores as a function of C\n",
    "plt.plot(C_array,training_array)\n",
    "plt.plot(C_array,test_array)\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "## 10. Making a parameter grid for GridSearchCV\n",
    "C_array = np.logspace(-4, -2, 100)\n",
    "tuning_C = {\"C\": C_array}\n",
    "\n",
    "## 11. Implementing GridSearchCV with l2 penalty\n",
    "clf_gs =  LogisticRegression()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(estimator= clf_gs, param_grid =tuning_C, scoring = 'f1', cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "## 12. Optimal C value and the score corresponding to it\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "\n",
    "## 13. Validating the \"best classifier\"\n",
    "clf_best_ridge =  LogisticRegression(C = gs.best_params_[\"C\"])\n",
    "clf_best_ridge.fit(X_train, y_train)\n",
    "y_pred_best = clf_best_ridge.predict(X_test)\n",
    "print(f1_score(y_test,y_pred_best))\n",
    "\n",
    "## 14. Implement L1 hyperparameter tuning with LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "C_array = np.logspace(-2,2,100)\n",
    "clf_l1 = LogisticRegressionCV(Cs=C_array, cv = 5, penalty = 'l1', scoring = 'f1', solver = 'liblinear')\n",
    "clf_l1.fit(X,y)\n",
    "\n",
    "## 15. Optimal C value and corresponding coefficients\n",
    "print('Best C value', clf_l1.C_)\n",
    "print('Best fit coefficients', clf_l1.coef_)\n",
    "\n",
    "\n",
    "## 16. Plotting the tuned L1 coefficients\n",
    "coefficients = clf_l1.coef_.ravel()\n",
    "coef = pd.Series(coefficients,predictors).sort_values()\n",
    " \n",
    "plt.figure(figsize = (12,8))\n",
    "coef.plot(kind='bar', title = 'Coefficients for tuned L1')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
